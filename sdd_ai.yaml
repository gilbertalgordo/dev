#include <SPI.h>
#include <SD.h>
#include <TensorFlowLite.h> // Example TinyML library header
#include <tensorflow/lite/micro/all_ops_resolver.h>
#include <tensorflow/lite/micro/micro_interpreter.h>
#include <tensorflow/lite/micro/micro_log.h>
#include <tensorflow/lite/micro/system_setup.h>
#include <tensorflow/lite/schema/schema_generated.h>

// SD Card Configuration
const int SD_CS_PIN = 10;
const char* MODEL_FILENAME = "model.tflite";

// TensorFlow Lite for Microcontrollers variables
namespace tflite {
    // Arena size (adjust based on your model size)
    const int kTensorArenaSize = 8 * 1024;
    uint8_t tensor_arena[kTensorArenaSize];
    MicroInterpreter* interpreter = nullptr;
    TfLiteTensor* input = nullptr;
    TfLiteTensor* output = nullptr;
    const Model* model = nullptr;
}

// Function to read the model file from the SD card into a buffer
bool load_model_from_sd_card(const char* filename, uint8_t* buffer, size_t max_size, size_t* actual_size) {
    if (!SD.begin(SD_CS_PIN)) {
        Serial.println("SD Card initialization failed!");
        return false;
    }
    
    File modelFile = SD.open(filename);
    if (!modelFile) {
        Serial.println("Failed to open model file!");
        return false;
    }

    *actual_size = modelFile.read(buffer, max_size);
    modelFile.close();
    
    if (*actual_size == 0) {
        Serial.println("Model file is empty!");
        return false;
    }
    
    return true;
}

void setup() {
    Serial.begin(115200);
    delay(1000); // Wait for Serial to initialize

    // 1. Load the Model from SD Card
    size_t model_size = 0;
    // You would dynamically allocate this buffer for a large model, 
    // but here we use a fixed-size array for simplicity.
    const size_t MAX_MODEL_BUFFER_SIZE = 100 * 1024; // 100KB example
    static uint8_t model_buffer[MAX_MODEL_BUFFER_SIZE]; 

    if (!load_model_from_sd_card(MODEL_FILENAME, model_buffer, MAX_MODEL_BUFFER_SIZE, &model_size)) {
        // Halt if model cannot be loaded
        while (1) { delay(1000); } 
    }
    Serial.printf("Model loaded, size: %lu bytes\n", model_size);

    // 2. Initialize the TinyML Interpreter
    tflite::model = tflite::GetModel(model_buffer);
    if (tflite::model->version() != TFLITE_SCHEMA_VERSION) {
        Serial.println("Model version mismatch!");
        while (1) { delay(1000); }
    }

    static tflite::AllOpsResolver resolver;
    static tflite::MicroInterpreter static_interpreter(
        tflite::model, resolver, tflite::tensor_arena, tflite::kTensorArenaSize);
    tflite::interpreter = &static_interpreter;

    // Allocate tensors for the model's intermediate data
    TfLiteStatus allocate_status = tflite::interpreter->AllocateTensors();
    if (allocate_status != kTfLiteOk) {
        Serial.println("Tensor allocation failed!");
        while (1) { delay(1000); }
    }

    // Get pointers to the input and output tensors
    tflite::input = tflite::interpreter->input(0);
    tflite::output = tflite::interpreter->output(0);

    Serial.println("AI Model setup complete!");
}

void loop() {
    // 3. Gather Sensor Data (The AI Input)
    // In a real application, you'd read from an accelerometer, microphone, etc.
    float sensor_data = analogRead(A0) / 1023.0f; 

    // 4. Populate the Input Tensor
    // Assuming a simple model with a single float input
    tflite::input->data.f[0] = sensor_data; 
    
    // 5. Run Inference
    TfLiteStatus invoke_status = tflite::interpreter->Invoke();
    if (invoke_status != kTfLiteOk) {
        Serial.println("Invoke failed!");
        return;
    }

    // 6. Read and Process the Output Tensor
    float ai_prediction = tflite::output->data.f[0];

    // Example: Print the prediction
    Serial.printf("Input: %.2f, Prediction: %.2f\n", sensor_data, ai_prediction);

    delay(100); 
}




// File: /config/model_cfg.json (on the SD card)
{
    "model_name": "anomaly_detector_v2.tflite",
    "threshold": 0.95,
    "log_interval_ms": 5000,
    "feature_scale": 100.0,
    "logging_enabled": true
}



#include <Arduino.h>
#include <SPI.h>
#include <SD.h>
// Use the official ESP-IDF JSON library for advanced parsing
#include <ArduinoJson.h> 

// Placeholder for TinyML Libraries (e.g., TensorFlow Lite Micro)
#include "TinyML_Inference.h" 

// --- Configuration Struct and Constants ---
struct Config {
    String model_name;
    float threshold;
    long log_interval_ms;
    float feature_scale;
    bool logging_enabled;
};

Config current_config;
const int SD_CS_PIN = 5;
const char* CONFIG_FILE = "/config/model_cfg.json";
const char* LOG_FILE_PATH = "/logs/inference_log.csv";

// --- Advanced Utility Functions ---

/**
 * @brief Reads and parses configuration from the SD card using JSON.
 */
bool loadConfiguration() {
    if (!SD.begin(SD_CS_PIN)) {
        Serial.println("SD Card Mount Failed!");
        return false;
    }

    File configFile = SD.open(CONFIG_FILE);
    if (!configFile) {
        Serial.println("Failed to open config file. Using defaults.");
        // Set fallback defaults if file is missing
        current_config = {"default_v1.tflite", 0.85, 10000, 50.0, true};
        return false;
    }

    // Allocate a buffer for the JSON document
    StaticJsonDocument<512> doc;
    DeserializationError error = deserializeJson(doc, configFile);
    configFile.close();

    if (error) {
        Serial.print("Failed to read JSON config: ");
        Serial.println(error.f_str());
        return false;
    }

    // Map JSON values to the Config struct
    current_config.model_name = doc["model_name"] | "default_v1.tflite";
    current_config.threshold = doc["threshold"] | 0.85;
    current_config.log_interval_ms = doc["log_interval_ms"] | 10000;
    current_config.feature_scale = doc["feature_scale"] | 50.0;
    current_config.logging_enabled = doc["logging_enabled"] | false;

    Serial.println("Configuration loaded successfully.");
    return true;
}

/**
 * @brief Logs the time, sensor data, and prediction result to the SD card.
 */
void logInferenceResult(float input_value, float prediction, bool anomaly) {
    if (!current_config.logging_enabled) return;

    File logFile = SD.open(LOG_FILE_PATH, FILE_APPEND);
    if (!logFile) {
        Serial.println("Failed to open log file for appending!");
        return;
    }

    // Format: Timestamp, RawValue, Prediction, AnomalyFlag
    String logEntry = String(millis()) + "," + 
                      String(input_value, 4) + "," + 
                      String(prediction, 4) + "," + 
                      (anomaly ? "1" : "0") + "\n";
                      
    logFile.print(logEntry);
    logFile.close();
}

// --- Main Program Flow ---

void setup() {
    Serial.begin(115200);
    delay(1000);
    Serial.println("--- Advanced SD Card AI Booting ---");

    // 1. Load Configuration
    if (loadConfiguration()) {
        Serial.printf("Active Model: %s\n", current_config.model_name.c_str());
        
        // 2. Initialize Model based on config (The TinyML_Inference part)
        // In a real scenario, this function would handle loading the specific 
        // .tflite file from the SD card and initializing the interpreter.
        bool model_ready = TinyML::loadModel(
            current_config.model_name, 
            current_config.feature_scale
        );
        
        if (!model_ready) {
            Serial.println("FATAL: Model initialization failed. Halting.");
            while(1);
        }

        // 3. Initialize Log File (Header)
        File logFile = SD.open(LOG_FILE_PATH, FILE_WRITE);
        if (logFile) {
             logFile.println("Timestamp_ms,RawValue,Prediction,Anomaly");
             logFile.close();
        }
    } else {
        Serial.println("Configuration load failed. Running in SAFE mode.");
        // Logic for a safe/fallback operational mode
    }
}

void loop() {
    static unsigned long last_log_time = 0;
    
    // Simulate reading a raw sensor value
    float raw_sensor_value = analogRead(A0); 
    
    // Pre-process the input using a parameter from the configuration
    float processed_input = raw_sensor_value / current_config.feature_scale;
    
    // 1. Run Inference (Assuming TinyML::runInference populates an output)
    float prediction = TinyML::runInference(processed_input);

    // 2. Anomaly Detection Logic
    bool is_anomaly = (prediction > current_config.threshold);
    
    // 3. Log data based on configured interval and flag
    if (millis() - last_log_time >= current_config.log_interval_ms) {
        logInferenceResult(raw_sensor_value, prediction, is_anomaly);
        last_log_time = millis();
        
        // Output for HUD/debug instance (as requested)
        Serial.printf("HUD: In=%.2f, Pred=%.4f, Anomaly=%s (Thresh=%.2f)\n", 
                      raw_sensor_value, prediction, 
                      is_anomaly ? "**YES**" : "No", 
                      current_config.threshold);
    }
    
    // Short delay to prevent crashing the loop
    delay(100); 
}



// File: TinyML_Inference.h (Conceptual Header)
namespace TinyML {
    // Function to load the model file and set scaling parameters
    bool loadModel(String model_name, float scale_factor);
    
    // Function to run the TFLite interpreter and return the output
    float runInference(float input_feature);
}
