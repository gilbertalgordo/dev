import random
from collections import defaultdict, Counter

class SimplePredictiveAI:
    def __init__(self, n=2):
        # n-gram size (n=2 means look at the previous 2 words)
        self.n = n 
        # Dictionary to store the model: { (word1, word2): [list of next words] }
        self.model = defaultdict(lambda: defaultdict(int))

    def train(self, corpus_text):
        """
        Trains the model on a given text corpus.
        """
        # 1. Preprocessing: Normalize, tokenize, and add start/end markers
        text = corpus_text.lower().replace('.', ' .').replace(',', ' ,').replace('?', ' ?').replace('!', ' !')
        words = text.split()
        
        # Add a special start marker to help the model begin a sentence
        padded_words = ['<START>'] * (self.n - 1) + words 

        # 2. Build the N-gram Model
        for i in range(len(padded_words) - self.n):
            # The 'prefix' (the n-1 words leading up to the target)
            prefix = tuple(padded_words[i:i + self.n - 1])
            # The 'target' (the next word)
            target = padded_words[i + self.n - 1]
            
            # Count the occurrences of the target word following the prefix
            self.model[prefix][target] += 1
    
    def predict_next_word(self, current_text, top_k=3):
        """
        Predicts the top 'k' most likely next words.
        """
        # 1. Get the last n-1 words typed by the user
        words = current_text.lower().split()
        
        # Handle cases where the input is too short
        if len(words) < self.n - 1:
            prefix = tuple(['<START>'] * ((self.n - 1) - len(words)) + words)
        else:
            prefix = tuple(words[-(self.n - 1):])

        # 2. Retrieve all next word counts for the current prefix
        next_words_counts = self.model.get(prefix)
        
        if not next_words_counts:
            # Fallback if the prefix hasn't been seen in the training data
            return ["the", "a", "I"] # Simple default suggestions

        # 3. Find the top K most frequent next words
        suggestions = Counter(next_words_counts).most_common(top_k)
        
        # Return only the words
        return [word for word, count in suggestions]

# --- Instance and Usage ---
# Use the "prefer instances" instruction
keyboard_ai_instance = SimplePredictiveAI(n=3)

# Sample training text (A small corpus for demonstration)
corpus = """
Hello how are you doing today? I am doing great! 
How is the weather? The weather is sunny and warm.
I am going to the park. I will bring a ball to play. 
I love to play sports. I love the sun.
"""

print("### Training the AI Model... ###")
keyboard_ai_instance.train(corpus)

print("\n--- Testing Predictions ---")

# Test 1: Full context available
input_text_1 = "I am going to"
predictions_1 = keyboard_ai_instance.predict_next_word(input_text_1)
print(f"Input: '{input_text_1}' -> Suggestions: {predictions_1}")

# Test 2: Different context
input_text_2 = "I love"
predictions_2 = keyboard_ai_instance.predict_next_word(input_text_2)
print(f"Input: '{input_text_2}' -> Suggestions: {predictions_2}")

# Test 3: Short input (uses <START> padding)
input_text_3 = "hello"
predictions_3 = keyboard_ai_instance.predict_next_word(input_text_3)
print(f"Input: '{input_text_3}' -> Suggestions: {predictions_3}")



import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences

# --- Instance Variables (Following user preference for 'instances') ---
# A larger corpus would be used in a real application
CORPUS = """
The quick brown fox jumps over the lazy dog.
I am going to the park today to play ball with the dog.
She loves the warm sun and the quick wind.
What is the weather like today? I am not sure.
"""
SEQUENCE_LENGTH = 5  # The model will look at 4 words to predict the 5th
EMBEDDING_DIM = 100  # Size of the word vector
LSTM_UNITS = 150     # Number of LSTM units (memory cells)

# 1. Tokenization and Vocabulary
tokenizer_instance = Tokenizer()
tokenizer_instance.fit_on_texts([CORPUS])
word_index_map = tokenizer_instance.word_index
VOCAB_SIZE = len(word_index_map) + 1 # +1 for the padding token (0)

# 2. Sequence Creation
input_sequences = []
# Create a sequence of word indices (e.g., [1, 5, 12, 8, 3] for 5 words)
token_list = tokenizer_instance.texts_to_sequences([CORPUS])[0]

# Generate sequences of length SEQUENCE_LENGTH
for i in range(SEQUENCE_LENGTH, len(token_list)):
    n_gram_sequence = token_list[i-SEQUENCE_LENGTH:i]
    input_sequences.append(n_gram_sequence)

# 3. Data Preparation (X: input, Y: target)
# Convert the list of sequences into a NumPy array
sequences_array = np.array(input_sequences)

# X is all words EXCEPT the last one in each sequence (the context)
X = sequences_array[:, :-1]
# Y is the LAST word in each sequence (the word to predict)
y = sequences_array[:, -1]

# 4. One-Hot Encoding the Target (Y)
# The prediction is a classification task over the entire vocabulary.
Y = to_categorical(y, num_classes=VOCAB_SIZE)
MAX_SEQUENCE_LENGTH = X.shape[1]

print(f"Vocabulary Size: {VOCAB_SIZE}")
print(f"Max Sequence Length (Input Length): {MAX_SEQUENCE_LENGTH}")




# Create the Sequential Model instance
advanced_keyboard_ai_model = Sequential()

# 1. Embedding Layer
# Converts integer word indices into dense vectors (Word Embeddings). 
# This helps the model understand semantic relationships between words.
advanced_keyboard_ai_model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))
[attachment_0](attachment)

# 2. LSTM Layer
# The core recurrent layer. It uses internal 'gates' (Input, Forget, Output) 
# to selectively remember or forget information across the sequence, 
# resolving the "vanishing gradient" problem of simple RNNs.
advanced_keyboard_ai_model.add(LSTM(LSTM_UNITS))

# 3. Dropout
# Regularization technique to prevent overfitting by randomly setting a fraction 
# of input units to 0 at each update during training.
advanced_keyboard_ai_model.add(Dropout(0.2))

# 4. Dense Output Layer
# Maps the LSTM output to the size of the vocabulary.
advanced_keyboard_ai_model.add(Dense(VOCAB_SIZE, activation='softmax'))

# 5. Compile the Model
advanced_keyboard_ai_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

print("\n### LSTM Model Summary (HUD) ###")
# Use model.summary() to see the architecture (HUD-like output)
advanced_keyboard_ai_model.summary() 



# This is a conceptual training step. Training on a real dataset takes minutes/hours.
# advanced_keyboard_ai_model.fit(X, Y, epochs=100, verbose=1) 
# For demonstration, we'll assume a trained model and proceed to prediction.

def predict_next_word_lstm(model, tokenizer, max_len, seed_text, top_k=3):
    """
    Predicts the top k most likely next words using the trained LSTM model.
    """
    token_list = tokenizer.texts_to_sequences([seed_text])[0]
    # Pad the sequence to match the model's expected input length
    token_list = pad_sequences([token_list], maxlen=max_len, padding='pre')
    
    # Get the probability distribution over all words in the vocabulary
    probabilities = model.predict(token_list, verbose=0)[0]
    
    # Get the indices of the top K highest probabilities
    top_k_indices = np.argsort(probabilities)[::-1][:top_k]
    
    # Map the indices back to words
    predictions = [tokenizer.index_word.get(idx) for idx in top_k_indices if idx != 0]
    
    return predictions

# --- Prediction Example ---
SEED_TEXT = "I am going to the"
print("\n--- Next Word Prediction ---")

# Since the model is not actually trained here, predictions will be random, 
# but the code structure is accurate.
# FAKE PREDICTION to demonstrate logic for an instance
FAKE_MODEL_OUTPUT = np.zeros(VOCAB_SIZE)
# Manually setting probabilities for demonstration:
if word_index_map.get('park') and word_index_map.get('dog'):
    FAKE_MODEL_OUTPUT[word_index_map['park']] = 0.85
    FAKE_MODEL_OUTPUT[word_index_map['dog']] = 0.50
    FAKE_MODEL_OUTPUT[word_index_map['sun']] = 0.30

class MockTrainedModel:
    """Mock class to simulate a trained model's prediction."""
    def predict(self, *args, **kwargs):
        # The LSTM model outputs a 2D array: [[probabilities...]]
        return np.array([FAKE_MODEL_OUTPUT])

mock_model = MockTrainedModel()
predictions = predict_next_word_lstm(
    mock_model, 
    tokenizer_instance, 
    MAX_SEQUENCE_LENGTH, 
    SEED_TEXT
)

print(f"Input: '{SEED_TEXT}'")
print(f"Top {len(predictions)} Suggestions: {predictions}")
