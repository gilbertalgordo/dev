import numpy as np
import matplotlib.pyplot as plt

class RadarInstance:
    def __init__(self, range_max=100, fov=90):
        self.range_max = range_max
        self.fov = fov  # Field of View in degrees
        self.targets = []

    def add_target(self, distance, angle, velocity, rcs):
        """
        RCS = Radar Cross Section (how 'visible' the object is)
        """
        self.targets.append({
            'dist': distance, 
            'angle': np.radians(angle), 
            'vel': velocity, 
            'rcs': rcs
        })

    def simulate_scan(self, noise_level=0.1):
        """Simulates raw sensor data with Gaussian noise."""
        detections = []
        for t in self.targets:
            # Signal Strength calculation based on Inverse Square Law
            signal_strength = t['rcs'] / (t['dist']**2)
            if signal_strength > noise_level:
                # Add synthetic noise to the reading
                detected_dist = t['dist'] + np.random.normal(0, 0.5)
                detections.append([detected_dist, t['angle'], t['vel']])
        return np.array(detections)

# --- Instance Initialization ---
radar_unit = RadarInstance()
radar_unit.add_target(distance=65, angle=30, velocity=250, rcs=1.5) # Fast Jet
radar_unit.add_target(distance=30, angle=-15, velocity=15, rcs=0.5) # Drone



def classify_target(velocity, rcs):
    """Simple AI heuristic for target identification."""
    if velocity > 200:
        return "STRIKE_AIRCRAFT", "HIGH"
    elif velocity > 50 and rcs < 0.2:
        return "STEALTH_UAV", "CRITICAL"
    elif velocity < 20:
        return "STATIC_OBSTACLE", "LOW"
    else:
        return "UNKNOWN", "MEDIUM"



def render_hud(detections):
    fig = plt.figure(figsize=(8, 8), facecolor='black')
    ax = fig.add_subplot(111, polar=True)
    ax.set_facecolor('#001100')
    
    # HUD Styling
    ax.tick_params(colors='lime')
    ax.grid(color='lime', alpha=0.3)
    
    if len(detections) > 0:
        dist = detections[:, 0]
        angles = detections[:, 1]
        vels = detections[:, 2]
        
        for i in range(len(dist)):
            label, threat = classify_target(vels[i], 1.0)
            ax.scatter(angles[i], dist[i], color='red', s=100, marker='x')
            ax.text(angles[i], dist[i]+5, f"{label}\n{vels[i]}m/s", 
                    color='lime', fontsize=9, fontweight='bold')

    ax.set_ylim(0, 100)
    plt.title("AI RADAR INSTANCE v1.0 - ACTIVE SCAN", color='lime', pad=20)
    plt.show()

# Execute Scan
data = radar_unit.simulate_scan()
render_hud(data)



import numpy as np
from scipy import signal

def get_micro_doppler_signature(raw_signal, fs=1000):
    """
    Converts 1D Radar signal into a 2D Time-Frequency Spectrogram.
    This 'image' is what the AI actually 'sees'.
    """
    frequencies, times, spectrogram = signal.spectrogram(
        raw_signal, fs, nperseg=64, noverlap=32, return_onesided=True
    )
    # Log scale for better AI feature extraction
    return 10 * np.log10(spectrogram + 1e-10)




from tensorflow.keras import layers, models

def build_advanced_radar_ai():
    model = models.Sequential([
        # Input: (Frames, Height, Width, Channels)
        layers.Input(shape=(10, 64, 64, 1)), 
        
        # Spatial-Temporal Feature Extraction
        layers.ConvLSTM2D(32, (3, 3), padding='same', return_sequences=False, activation='relu'),
        layers.BatchNormalization(),
        
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(4, activation='softmax') # [DRONE, BIRD, PERSON, VEHICLE]
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy')
    return model

radar_classifier = build_advanced_radar_ai()



class KalmanTracker:
    def __init__(self, dt=0.1):
        # State: [pos_x, pos_y, vel_x, vel_y]
        self.x = np.zeros((4, 1))
        # Transition Matrix
        self.F = np.array([[1, 0, dt, 0],
                           [0, 1, 0, dt],
                           [0, 0, 1, 0],
                           [0, 0, 0, 1]])
        # Measurement Matrix (we only measure position)
        self.H = np.array([[1, 0, 0, 0],
                           [0, 1, 0, 0]])
        self.P = np.eye(4) * 100 # Uncertainty
        self.R = np.eye(2) * 1   # Measurement Noise

    def predict(self):
        self.x = np.dot(self.F, self.x)
        self.P = np.dot(np.dot(self.F, self.P), self.F.T)
        return self.x

    def update(self, z):
        # Kalman Gain
        S = np.dot(self.H, np.dot(self.P, self.H.T)) + self.R
        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))
        # Update State
        y = z - np.dot(self.H, self.x)
        self.x = self.x + np.dot(K, y)
        self.P = self.P - np.dot(np.dot(K, self.H), self.P)



# Initialization
tracker = KalmanTracker()
# ... (simulate incoming radar data stream) ...

def process_frame(raw_iq_data):
    # 1. AI Classification
    spec = get_micro_doppler_signature(raw_iq_data)
    # prediction = radar_classifier.predict(spec) 
    
    # 2. State Estimation (Tracking)
    # Assume z is [detected_x, detected_y]
    z = np.array([[50.5], [20.2]]) 
    tracker.update(z)
    future_state = tracker.predict()
    
    return {
        "id": "TGT-01",
        "class": "STRIKE_UAV",
        "pos": (future_state[0,0], future_state[1,0]),
        "vel": (future_state[2,0], future_state[3,0])
    }
