import time
import random

class DynamicPricingAI:
    """
    A conceptual AI model for dynamic item pricing.
    It simulates pricing based on cost, demand, and inventory.
    """

    def __init__(self, item_name, base_cost, initial_inventory, item_category="General"):
        # Core Item Data
        self.item_name = item_name
        self.base_cost = base_cost  # Manufacturing/Acquisition cost
        self.inventory = initial_inventory
        self.item_category = item_category

        # Pricing Levers
        self.target_margin_rate = 0.40  # Target 40% margin
        self.demand_factor = 1.0       # Current perceived demand multiplier (1.0 is neutral)

    def _assess_demand(self):
        """
        Simulates demand assessment based on external factors (e.g., time, category).
        In a real AI, this would come from sales data, competitor prices, etc.
        """
        if self.item_category == "Seasonal":
            # Higher demand simulation for seasonal items
            self.demand_factor = 1.25
        elif self.inventory < 10:
            # Low inventory = Price up (scarcity)
            self.demand_factor = 1.10
        else:
            # Random slight fluctuation for generality
            self.demand_factor = 1.0 + random.uniform(-0.05, 0.05)

    def calculate_suggested_retail_price(self):
        """
        Calculates the preliminary Suggested Retail Price (SRP) based on cost and margin.
        """
        self._assess_demand() # Update demand factor before pricing

        # 1. Base Price Calculation (Cost + Margin)
        base_price_srp = self.base_cost / (1 - self.target_margin_rate)
        
        # 2. Apply Demand Adjustment
        srp = base_price_srp * self.demand_factor

        # Rounding for clean pricing (e.g., ending in .99)
        srp = round(srp, 2)
        
        # Ensure price is never below cost
        if srp < self.base_cost:
            srp = self.base_cost * 1.05 
            
        return srp

    def calculate_dynamic_discount(self, current_srp):
        """
        Determines the appropriate discount based on inventory and time.
        Returns the discount percentage (0.0 to 1.0).
        """
        discount_rate = 0.0

        if self.inventory > 50:
            # High inventory, aggressive discount to move units
            discount_rate += 0.20
        elif self.inventory > 20 and self.inventory <= 50:
            # Moderate inventory, small discount
            discount_rate += 0.10
        elif self.item_category == "Clearance":
            # Category-specific discount
            discount_rate += 0.30

        # Cap the discount rate for profitability
        if discount_rate > 0.40:
            discount_rate = 0.40

        return round(discount_rate, 2)

    def generate_pricing_data(self):
        """
        Generates the final pricing data instance.
        """
        # Calculate the SRP first
        srp = self.calculate_suggested_retail_price()
        
        # Determine the discount based on the SRP and other factors
        discount_rate = self.calculate_dynamic_discount(srp)
        
        # Calculate the final selling price
        final_price = srp * (1 - discount_rate)

        # Output the complete pricing instance
        pricing_instance = {
            "Item": self.item_name,
            "Category": self.item_category,
            "Base_Cost": f"${self.base_cost:.2f}",
            "Suggested_Retail_Price": f"${srp:.2f}",
            "Dynamic_Discount_Rate": f"{discount_rate * 100:.0f}%",
            "Final_Selling_Price": f"${final_price:.2f}",
            "Inventory_Level": self.inventory,
            "Time_Stamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return pricing_instance

# --- Example Usage (Instances) ---

print("## ðŸ›’ Dynamic Pricing Instances Generated by AI")
print("---")

# Instance 1: High Demand, Low Inventory Item
ai_model_A = DynamicPricingAI(
    item_name="Smart Watch X10", 
    base_cost=150.00, 
    initial_inventory=5, # Low Inventory = Higher Price
    item_category="Tech"
)
print("Instance A (Low Stock, High Demand):")
print(ai_model_A.generate_pricing_data())

print("\n---")

# Instance 2: Normal Item, High Inventory = Discounted
ai_model_B = DynamicPricingAI(
    item_name="Premium Coffee Mug", 
    base_cost=5.00, 
    initial_inventory=70, # High Inventory = Higher Discount
    item_category="General"
)
print("Instance B (High Stock, Moderate Demand):")
print(ai_model_B.generate_pricing_data())

print("\n---")

# Instance 3: Clearance Item
ai_model_C = DynamicPricingAI(
    item_name="Winter Gloves (End of Season)", 
    base_cost=10.00, 
    initial_inventory=30, 
    item_category="Clearance" # Clearance Category = Significant Discount
)
print("Instance C (Clearance):")
print(ai_model_C.generate_pricing_data())



import numpy as np
import pandas as pd
import random

# --- 1. ENVIRONMENT (The 'World' where the AI acts) ---

class PricingEnvironment:
    """
    Simulates the market environment for a single product.
    Includes cost, demand elasticity, and inventory.
    """
    def __init__(self, item_cost, base_demand, elasticity=-0.8, max_inventory=100):
        self.item_cost = item_cost
        self.base_demand = base_demand
        self.elasticity = elasticity # Measures price sensitivity (e.g., -0.8 is typical)
        self.max_inventory = max_inventory
        self.current_inventory = max_inventory
        self.min_price = item_cost * 1.05 # Minimum 5% margin
        self.max_price = item_cost * 2.50 # Maximum 150% margin
        
    def _calculate_demand(self, price):
        """
        Calculates demand based on the price using an exponential demand model.
        A lower price increases demand. Low inventory can suppress demand.
        """
        # Demand formula: Demand = Base_Demand * (Price / Suggested_Retail_Price)^Elasticity
        # Use a nominal price (e.g., min_price) as the reference for simplicity
        demand = self.base_demand * ((price / self.min_price) ** self.elasticity)
        
        # Adjust demand downwards if inventory is critically low
        if self.current_inventory < 10:
            demand *= (self.current_inventory / 10) # Linearly decrease demand
        
        return int(max(0, demand)) # Demand cannot be negative

    def get_profit_and_update(self, action_price):
        """
        The main interaction loop: set price, calculate sales/profit, update state.
        """
        # Determine actual sales (cannot sell more than inventory)
        demand = self._calculate_demand(action_price)
        units_sold = min(demand, self.current_inventory)
        
        # Calculate Reward (Profit)
        profit = units_sold * (action_price - self.item_cost)
        
        # Update State (Inventory)
        self.current_inventory -= units_sold
        
        # Return Reward (Profit) and the actual number of units sold
        return profit, units_sold

# --- 2. AGENT (The 'AI' that makes the pricing decision) ---

class QLearningAgent:
    """
    An AI agent that uses Q-Learning to find the optimal price (action) 
    that maximizes the cumulative profit (reward).
    """
    def __init__(self, env: PricingEnvironment, pricing_steps=10):
        self.env = env
        # Define the discrete price actions the agent can choose from
        self.actions = np.linspace(env.min_price, env.max_price, pricing_steps)
        self.actions = np.round(self.actions, 2)
        
        # State space is simplified: [Inventory_Level]
        # Discretize inventory into buckets: High (3), Medium (2), Low (1)
        self.state_levels = {
            'High': 3, 
            'Medium': 2, 
            'Low': 1
        }
        self.q_table = {} # Q(State, Action) = Expected Future Reward
        
        # RL Hyperparameters
        self.learning_rate = 0.1   # Alpha: How quickly the agent adapts
        self.discount_factor = 0.9 # Gamma: How much future reward is valued
        self.exploration_prob = 1.0 # Epsilon: Probability of exploring vs. exploiting

    def _get_current_state(self):
        """Maps continuous inventory to a discrete state (High, Medium, Low)."""
        inv = self.env.current_inventory
        if inv >= self.env.max_inventory * 0.7:
            return 'High'
        elif inv >= self.env.max_inventory * 0.2:
            return 'Medium'
        else:
            return 'Low'

    def _get_q_value(self, state, action):
        return self.q_table.get((state, action), 0.0)

    def choose_action(self):
        """Epsilon-greedy strategy to balance exploration and exploitation."""
        state = self._get_current_state()
        
        # Decaying Epsilon: Decrease exploration over time
        self.exploration_prob = max(0.01, self.exploration_prob * 0.999) 

        if random.random() < self.exploration_prob:
            # Explore: Choose a random price
            return random.choice(self.actions)
        else:
            # Exploit: Choose the price with the highest Q-value for the current state
            q_values = {
                action: self._get_q_value(state, action)
                for action in self.actions
            }
            # Handle ties randomly
            max_q = max(q_values.values())
            best_actions = [a for a, q in q_values.items() if q == max_q]
            return random.choice(best_actions)

    def update_q_value(self, state, action, reward, next_state):
        """
        The Q-Learning update rule (Bellman Equation).
        $$Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha (R + \gamma \max_{a'} Q(s', a'))$$
        """
        old_q = self._get_q_value(state, action)
        
        # Find the max Q-value for the next state (s')
        next_q_values = [self._get_q_value(next_state, a) for a in self.actions]
        max_future_q = max(next_q_values)
        
        # New Q-value calculation
        new_q = old_q + self.learning_rate * (reward + self.discount_factor * max_future_q - old_q)
        self.q_table[(state, action)] = new_q
        
    def train(self, episodes=1000):
        """Simulates price-setting over many episodes to learn the optimal policy."""
        print(f"Starting Q-Learning training for {episodes} episodes...")
        for episode in range(episodes):
            current_state = self._get_current_state()
            action_price = self.choose_action()
            
            # Interact with the environment
            reward, _ = self.env.get_profit_and_update(action_price)
            next_state = self._get_current_state()
            
            # Update the agent's knowledge
            self.update_q_value(current_state, action_price, reward, next_state)
            
            if episode % 100 == 0:
                print(f"Episode {episode}/{episodes}. Inventory: {self.env.current_inventory}, Epsilon: {self.exploration_prob:.3f}")
            
            # Reset environment after a fixed number of steps or if inventory is depleted
            if episode % 500 == 0 and episode != 0:
                self.env.current_inventory = self.env.max_inventory
        print("Training complete.")

# --- 3. PRICE GENERATOR (Final Output) ---

def generate_pricing_instance(agent: QLearningAgent, item_name, item_cost):
    """
    Uses the trained Q-Learning policy to generate a final pricing instance.
    The agent chooses the Final Selling Price. SRP is calculated retroactively.
    """
    state = agent._get_current_state()
    
    # 1. The agent's learned optimal price (Exploit mode for final decision)
    q_values = {a: agent._get_q_value(state, a) for a in agent.actions}
    
    # Use the max Q-value to find the optimal *Final Selling Price*
    max_q = max(q_values.values())
    final_selling_price = random.choice([a for a, q in q_values.items() if q == max_q])
    
    # 2. Suggested Retail Price (SRP) - A Cost-Plus Benchmark
    # Set SRP as the theoretical price needed for a typical 40% margin
    srp = item_cost / (1 - 0.40) 
    srp = round(srp, 2)
    
    # 3. Dynamic Discount Calculation
    # The discount is the difference between the SRP and the AI's optimal price
    discount_amount = srp - final_selling_price
    discount_rate = discount_amount / srp if srp > final_selling_price else 0.0
    
    # --- Output Instance ---
    pricing_instance = {
        "Item": item_name,
        "Current_Inventory_State": state,
        "Base_Cost": f"${item_cost:.2f}",
        "Suggested_Retail_Price_SRP": f"${srp:.2f}",
        "AI_Optimal_Selling_Price": f"${final_selling_price:.2f}",
        "Dynamic_Discount_Rate": f"{discount_rate * 100:.1f}%",
        "Profit_Margin_at_Optimal_Price": f"{(final_selling_price - item_cost) / final_selling_price * 100:.1f}%"
    }
    return pricing_instance

# --- EXECUTION (Generating the Instances) ---

print("## ðŸš€ Q-Learning Dynamic Pricing AI")
print("---")

# 1. Initialize the Environment and Agent
ITEM_NAME = "Zenith X-Gaming Headset"
ITEM_COST = 80.00
BASE_DEMAND = 50
MAX_INVENTORY = 200

env = PricingEnvironment(
    item_cost=ITEM_COST, 
    base_demand=BASE_DEMAND, 
    elasticity=-1.2, # Highly elastic, very sensitive to price
    max_inventory=MAX_INVENTORY
)
agent = QLearningAgent(env, pricing_steps=15)

# 2. Train the AI
# This is the 'learning' phase where the AI optimizes the Q-table
agent.train(episodes=5000)

# 3. Generate Final Pricing Instance
print("\n---")
print("### ðŸ“Š Final Price Decision (Instance)")
# The final price is based on the optimal learned Q-values
final_pricing = generate_pricing_instance(agent, ITEM_NAME, ITEM_COST)
print(pd.Series(final_pricing).to_string())

# Simulate low inventory to show a different pricing instance
env.current_inventory = 10 
print("\n### ðŸ“‰ Low Inventory Price Decision (New Instance)")
# The AI should now choose a higher price because the state is 'Low'
low_stock_pricing = generate_pricing_instance(agent, ITEM_NAME, ITEM_COST)
print(pd.Series(low_stock_pricing).to_string())
